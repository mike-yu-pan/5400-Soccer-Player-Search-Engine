from newsapi import NewsApiClient
from google.cloud import bigquery
import os

# read API key and establish a newsapi client
with open("newsapi_key.txt", "r") as file: 
  api_key = file.read().strip()
file.close()
newsapi = NewsApiClient(api_key=api_key)

# establish a bigquery client
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = 'cloud_service_account.json'
client = bigquery.Client()

# Extract data from the News API 
def extract_data(query: str, language="en", page_size=3, max_pages=2):
    # this function takes in a string, and send the string to newsapi
    # to request 3(default) articles
    all_articles = []
    page = 1
    while True:
        articles = newsapi.get_everything(q=query, language=language, page_size=page_size, page=page)
        if not articles['articles'] or page >= max_pages:
            break
        all_articles.extend(articles['articles'])
        page += 1
    return all_articles


# Transform the data into a suitable format
def transform_data(articles:list, search_query:str):
    # this function takes in a list of articles generated by extract_data() and the 
    # query as the key word to transform the data into a list of json object to be
    # loaded into big query. This way the data can match our schema in the database
    transformed_articles = []
    for article in articles:
        transformed_articles.append(
            {
                "key_word": search_query,
                "source_id": article["source"]["id"],
                "source_name": article["source"]["name"],
                "author": article["author"],
                "title": article["title"],
                "description": article["description"],
                "url": article["url"],
                "url_to_image": article["urlToImage"],
                "published_at": article["publishedAt"],
                "content": article["content"],
            }
        )
    return transformed_articles

# loading the data into bigquery
def load_data_to_bigquery(transformed_articles:list, dataset_id:str, table_id:str):
    # This function uploads the transformed data to bigquery

    # Load the data into the table
    table_ref = client.dataset(dataset_id).table(table_id)
    table = client.get_table(table_ref)

    errors = client.insert_rows_json(table, transformed_articles)
    if errors:
        raise RuntimeError("Failed to load data to BigQuery: {}".format(errors))