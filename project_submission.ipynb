{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 6 Term Project Implementation\n",
    "- Yu Pan\n",
    "- Benjamin Wechsler \n",
    "- Chloe Wang \n",
    "- Xinyu Wu \n",
    "- Yaoze Liu \n",
    "\n",
    "Note that the original code was written in three modules: \n",
    "1. _news_etl.py_: this module creates a data pipeline that pulls, transforms and load data into the data base on Bigquery. \n",
    "2. _query.py_: this module contains the query strings and make requests through Bigquery API\n",
    "3. _app.py_: this module contains the main body of the web application, when the search() dunction is called, the news data ETL pipeline is activated and two queries(one query for the FIFA stats data and one for the news data)\n",
    "\n",
    "For the ease of your grading, the above modules are combined here and the grading points are specified below: \n",
    "- Design of the back-end data warehouse(s): see _news_etl.py_ and _query.py_\n",
    "- Data ETL process pipeline: see _news_etl.py_\n",
    "- Interface to the designed system: see _app.py_\n",
    "- Inclusion of at least 3 technologies studied in the course: \n",
    "  - Google cloud as data warehouse\n",
    "  - Bigquery for relational database \n",
    "  - Flask for user interface\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the functions required for the News data pipeline. This is originally from _news_etl.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# read API key and establish a newsapi client\n",
    "with open(\"newsapi_key.txt\", \"r\") as file: \n",
    "  api_key = file.read().strip()\n",
    "file.close()\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "# establish a bigquery client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'cloud_service_account.json'\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Extract data from the News API \n",
    "def extract_data(query: str, language=\"en\", page_size=3, max_pages=2):\n",
    "    # this function takes in a string, and send the string to newsapi\n",
    "    # to request 3(default) articles\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        articles = newsapi.get_everything(q=query, language=language, page_size=page_size, page=page)\n",
    "        if not articles['articles'] or page >= max_pages:\n",
    "            break\n",
    "        all_articles.extend(articles['articles'])\n",
    "        page += 1\n",
    "    return all_articles\n",
    "\n",
    "\n",
    "# Transform the data into a suitable format\n",
    "def transform_data(articles, search_query):\n",
    "    # this function takes in a list of articles generated by extract_data() and the \n",
    "    # query as the key word to transform the data into a list of json object to be\n",
    "    # loaded into big query. This way the data can match our schema in the database\n",
    "    transformed_articles = []\n",
    "    for article in articles:\n",
    "        transformed_articles.append(\n",
    "            {\n",
    "                \"key_word\": search_query,\n",
    "                \"source_id\": article[\"source\"][\"id\"],\n",
    "                \"source_name\": article[\"source\"][\"name\"],\n",
    "                \"author\": article[\"author\"],\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"url\": article[\"url\"],\n",
    "                \"url_to_image\": article[\"urlToImage\"],\n",
    "                \"published_at\": article[\"publishedAt\"],\n",
    "                \"content\": article[\"content\"],\n",
    "            }\n",
    "        )\n",
    "    return transformed_articles\n",
    "\n",
    "# loading the data into bigquery\n",
    "def load_data_to_bigquery(transformed_articles, dataset_id, table_id):\n",
    "    # This function uploads the transformed data to bigquery\n",
    "\n",
    "    # Load the data into the table\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    errors = client.insert_rows_json(table, transformed_articles)\n",
    "    if errors:\n",
    "        raise RuntimeError(\"Failed to load data to BigQuery: {}\".format(errors))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write the methods required for the search function and interaction with the data tables. This is originally from _query.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_by_player(name:str, info: list): \n",
    "  # the function takes in a name and search match it on bigquery. the info argument\n",
    "  # is a list of extra info that the user would like to see by checking the boxes\n",
    "  # in the web interface\n",
    "  \n",
    "  # convert info from list to string to append it into the query\n",
    "  info = \", \" + \", \".join(info)\n",
    "\n",
    "  query = f\"\"\"\n",
    "  SELECT DISTINCT fifa_version, long_name, age, value_eur, club_name, nationality_name, player_face_url {info}\n",
    "  FROM aerobic-goal-384015.soccer_player.player\n",
    "  WHERE LOWER(long_name) LIKE LOWER('%{name}%') \n",
    "  ORDER BY fifa_version DESC\n",
    "  \"\"\"\n",
    "\n",
    "  # run the query and return it as a dataframe\n",
    "  query_job = client.query(query)\n",
    "  return query_job.result().to_dataframe()\n",
    "\n",
    "def query_by_coach(name: str): \n",
    "  # takes in a coach's name and query it from the coach data set\n",
    "  \n",
    "  query = f\"\"\"\n",
    "  SELECT DISTINCT *\n",
    "  FROM aerobic-goal-384015.soccer_player.coach\n",
    "  WHERE LOWER(long_name) LIKE LOWER('%{name}%') \n",
    "  \"\"\"\n",
    "\n",
    "  # run the query and return it as a dataframe\n",
    "  query_job = client.query(query)\n",
    "  return query_job.result().to_dataframe()\n",
    "\n",
    "\n",
    "def query_by_team(team: str): \n",
    "  # takes in a team's name and query it from the team data set\n",
    "  \n",
    "  query = f\"\"\"\n",
    "  SELECT DISTINCT fifa_version, team_name, league_name, overall, coach_id\n",
    "  FROM aerobic-goal-384015.soccer_player.team\n",
    "  WHERE LOWER(team_name) LIKE LOWER('%{team}%')\n",
    "  ORDER BY fifa_version DESC\n",
    "  \"\"\"\n",
    "\n",
    "  # run the query and return it as a dataframe\n",
    "  query_job = client.query(query)\n",
    "  return query_job.result().to_dataframe()\n",
    "\n",
    "\n",
    "def query_by_rating(rating): \n",
    "  # takes in a rating and query it from the player data set\n",
    "  \n",
    "  query = f\"\"\"\n",
    "  SELECT DISTINCT fifa_version, long_name, age, value_eur, club_name, nationality_name, player_face_url\n",
    "  FROM aerobic-goal-384015.soccer_player.player\n",
    "  WHERE overall >= {rating}\n",
    "  ORDER BY fifa_version DESC\n",
    "  \"\"\"\n",
    "\n",
    "  # run the query and return it as a dataframe\n",
    "  query_job = client.query(query)\n",
    "  return query_job.result().to_dataframe()\n",
    "\n",
    "def query_news(name): \n",
    "  # takes in any topic and query it from the news data set\n",
    "  \n",
    "  query = f\"\"\"\n",
    "  SELECT DISTINCT key_word, source_name, title, description, content, url, published_at\n",
    "  FROM aerobic-goal-384015.soccer_player.news\n",
    "  WHERE key_word = '{name}'\n",
    "  ORDER BY published_at DESC\n",
    "  \"\"\"\n",
    "\n",
    "  # run the query and return it as a dataframe\n",
    "  query_job = client.query(query)\n",
    "  return query_job.result().to_dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, create the web application with Flask. This is originally from _app.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "#import query\n",
    "#import news_etl\n",
    "import cachetools\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set up a cache with a TTL of 12 hours and a maximum of 100 items\n",
    "# because the API is limited to 100 requests per 12 hours\n",
    "cache = cachetools.TTLCache(maxsize=100, ttl=12 * 60 * 60)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('search.html')\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search():\n",
    "    search_query = request.form.get('search_query')\n",
    "    search_by = request.form.get('search_by')\n",
    "    info = request.form.getlist('info')\n",
    "    \n",
    "    if search_by == \"player\": \n",
    "        df = query_by_player(search_query,info)\n",
    "    if search_by == \"coach\": \n",
    "        df = query_by_coach(search_query)\n",
    "    if search_by == \"team\": \n",
    "        df = query_by_team(search_query)\n",
    "    if search_by == \"rating\": \n",
    "        df = query_by_rating(search_query)\n",
    "\n",
    "    table_html = df.to_html(escape=False, formatters={\n",
    "        'player_face_url': lambda url: f'<img src=\"{url}\" width=\"100\" height=\"100\" alt=\"Player Face\">'}, index=False)\n",
    "\n",
    "    # Check if the articles for the search_query are cached, otherwise fetch and cache them\n",
    "    if search_query not in cache:\n",
    "        articles = extract_data(search_query)\n",
    "        transformed_articles = transform_data(articles, search_query)\n",
    "        load_data_to_bigquery(transformed_articles, \"soccer_player\", \"news\")\n",
    "        cache[search_query] = transformed_articles\n",
    "    \n",
    "    # query the just-loaded news data from the cloud, then display them on our website\n",
    "    news = query_news(search_query).to_html() \n",
    "\n",
    "    return \"Search results for: {} (search by: {})\".format(search_query, search_by) + table_html + \"Latest news about {}\".format(search_query) + news\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
